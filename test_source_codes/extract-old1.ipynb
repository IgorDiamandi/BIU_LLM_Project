{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be482133-4119-49c5-8bfa-28fec067605a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import boto3\n",
    "from langchain.llms import Anthropic\n",
    "from langchain_community.chat_models import ChatAnthropic\n",
    "import pytesseract\n",
    "import pdf2image\n",
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "import io\n",
    "import tempfile\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def load_config():\n",
    "    with open('config/config.yaml', 'r') as file:\n",
    "        return yaml.safe_load(file)\n",
    "\n",
    "def init_aws_client(config):\n",
    "    return boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id=config['aws']['aws_access_key_id'],\n",
    "        aws_secret_access_key=config['aws']['aws_secret_access_key'],\n",
    "        region_name=config['aws']['region_name']\n",
    "    )\n",
    "\n",
    "def init_claude(config):\n",
    "    return ChatAnthropic(\n",
    "        anthropic_api_key=config['anthropic']['claud_key'],\n",
    "        model=\"claude-3-sonnet-20240229\"\n",
    "    )\n",
    "\n",
    "def download_file(s3_client, bucket, key):\n",
    "    with tempfile.NamedTemporaryFile(delete=False) as tmp_file:\n",
    "        s3_client.download_file(bucket, key, tmp_file.name)\n",
    "        return tmp_file.name\n",
    "\n",
    "def extract_text_from_image(image):\n",
    "    \"\"\"Extract text from a PIL Image object using Tesseract with Hebrew support\"\"\"\n",
    "    # Convert PIL Image to OpenCV format\n",
    "    opencv_img = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # Image preprocessing\n",
    "    gray = cv2.cvtColor(opencv_img, cv2.COLOR_BGR2GRAY)\n",
    "    denoised = cv2.fastNlMeansDenoising(gray)\n",
    "    \n",
    "    # Adaptive thresholding\n",
    "    thresh = cv2.adaptiveThreshold(\n",
    "        denoised, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "        cv2.THRESH_BINARY, 11, 2\n",
    "    )\n",
    "    \n",
    "    # Convert back to PIL Image\n",
    "    processed_img = Image.fromarray(thresh)\n",
    "    \n",
    "    # Extract text with Tesseract, supporting both Hebrew and English\n",
    "    text = pytesseract.image_to_string(\n",
    "        processed_img, \n",
    "        lang='eng+heb',  # Use both English and Hebrew languages\n",
    "        config='--psm 3'  # Fully automatic page segmentation\n",
    "    )\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text and images from PDF\"\"\"\n",
    "    all_text = []\n",
    "    \n",
    "    # Open PDF with PyMuPDF\n",
    "    doc = fitz.open(pdf_path)\n",
    "    \n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc[page_num]\n",
    "        \n",
    "        # Extract text directly from PDF\n",
    "        text = page.get_text()\n",
    "        all_text.append(text)\n",
    "        \n",
    "        # Extract images from PDF\n",
    "        image_list = page.get_images()\n",
    "        \n",
    "        for img_index, img in enumerate(image_list):\n",
    "            try:\n",
    "                # Get image data\n",
    "                xref = img[0]\n",
    "                base_image = doc.extract_image(xref)\n",
    "                image_bytes = base_image[\"image\"]\n",
    "                \n",
    "                # Convert to PIL Image\n",
    "                image = Image.open(io.BytesIO(image_bytes))\n",
    "                \n",
    "                # Extract text from image\n",
    "                image_text = extract_text_from_image(image)\n",
    "                if image_text:\n",
    "                    all_text.append(image_text)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting image {img_index} from page {page_num}: {str(e)}\")\n",
    "                \n",
    "    doc.close()\n",
    "    return \"\\n\".join(all_text)\n",
    "\n",
    "def extract_text_from_image_file(image_path):\n",
    "    \"\"\"Extract text from image file\"\"\"\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            return extract_text_from_image(img)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image file: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "def analyze_with_claude(llm, text):\n",
    "    prompt = \"\"\"\n",
    "    Analyze this document and extract the following information in JSON format.\n",
    "    The text contains course syllabus information, likely in both Hebrew and English.\n",
    "    \n",
    "    Required JSON structure:\n",
    "    {\n",
    "        \"course_name\": \"Course name in original language\",\n",
    "        \"program_manager\": \"Look for 'מנהל התוכנית' or program manager\",\n",
    "        \"instructors\": [\n",
    "            {\n",
    "                \"name\": \"Instructor name\",\n",
    "                \"role\": \"Role (e.g., יועץ מקצועי, מרצה, מרצה בכיר, מדריך)\",\n",
    "                \"title\": \"Professional title if available\",\n",
    "                \"description\": \"Additional description or background\"\n",
    "            }\n",
    "        ],\n",
    "        \"summary\": \"A comprehensive summary of the course content\"\n",
    "    }\n",
    "\n",
    "    Keep all text in its original language (Hebrew or English).\n",
    "    Ensure proper extraction of Hebrew text and names.\n",
    "    If a field is not found, use null or empty array [].\n",
    "    \n",
    "    Document text:\n",
    "    {text}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = llm.complete(prompt.format(text=text[:15000]))  # Limit text length\n",
    "        return json.loads(response.completion)\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing with Claude: {str(e)}\")\n",
    "        return {\n",
    "            \"course_name\": \"\",\n",
    "            \"program_manager\": \"\",\n",
    "            \"instructors\": [],\n",
    "            \"summary\": \"Error analyzing content\"\n",
    "        }\n",
    "\n",
    "def process_files(config):\n",
    "    s3_client = init_aws_client(config)\n",
    "    llm = init_claude(config)\n",
    "    \n",
    "    # List all files in the upload bucket/path\n",
    "    paginator = s3_client.get_paginator('list_objects_v2')\n",
    "    pages = paginator.paginate(\n",
    "        Bucket=config['aws']['upload_bucket_name'],\n",
    "        Prefix=config['aws']['upload_path']\n",
    "    )\n",
    "\n",
    "    for page in pages:\n",
    "        for obj in page.get('Contents', []):\n",
    "            file_key = obj['Key']\n",
    "            file_name = os.path.basename(file_key)\n",
    "            file_ext = os.path.splitext(file_name)[1][1:].lower()\n",
    "            \n",
    "            if file_ext not in ['pdf', 'jpg', 'jpeg', 'png']:\n",
    "                continue\n",
    "\n",
    "            print(f\"Processing file: {file_name}\")\n",
    "            \n",
    "            # Download file\n",
    "            temp_path = download_file(\n",
    "                s3_client, \n",
    "                config['aws']['upload_bucket_name'], \n",
    "                file_key\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                # Extract text based on file type\n",
    "                if file_ext == 'pdf':\n",
    "                    extracted_text = extract_text_from_pdf(temp_path)\n",
    "                else:\n",
    "                    extracted_text = extract_text_from_image_file(temp_path)\n",
    "                \n",
    "                # Analyze with Claude\n",
    "                analysis = analyze_with_claude(llm, extracted_text)\n",
    "                \n",
    "                # Create result JSON\n",
    "                result = {\n",
    "                    \"file_name\": file_name,\n",
    "                    \"course_name\": analysis.get(\"course_name\", \"\"),\n",
    "                    \"program_manager\": analysis.get(\"program_manager\", \"\"),\n",
    "                    \"instructors\": analysis.get(\"instructors\", []),\n",
    "                    \"summary\": analysis.get(\"summary\", \"\"),\n",
    "                    \"text\": extracted_text\n",
    "                }\n",
    "\n",
    "                # Ensure proper instructor structure\n",
    "                for instructor in result[\"instructors\"]:\n",
    "                    if isinstance(instructor, dict):\n",
    "                        instructor.setdefault(\"name\", \"\")\n",
    "                        instructor.setdefault(\"role\", \"\")\n",
    "                        instructor.setdefault(\"title\", None)\n",
    "                        instructor.setdefault(\"description\", None)\n",
    "\n",
    "                # Save to S3\n",
    "                json_key = os.path.join(\n",
    "                    config['aws']['extract_txt_path'],\n",
    "                    f\"{os.path.splitext(file_name)[0]}.json\"\n",
    "                )\n",
    "                \n",
    "                s3_client.put_object(\n",
    "                    Bucket=config['aws']['txt_extract_bucket_name'],\n",
    "                    Key=json_key,\n",
    "                    Body=json.dumps(result, ensure_ascii=False, indent=2).encode('utf-8')\n",
    "                )\n",
    "\n",
    "                print(f\"Successfully processed and saved: {file_name}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_name}: {str(e)}\")\n",
    "            \n",
    "            finally:\n",
    "                # Cleanup\n",
    "                if os.path.exists(temp_path):\n",
    "                    os.remove(temp_path)\n",
    "\n",
    "def main():\n",
    "    config = load_config()\n",
    "    process_files(config)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_Projects",
   "language": "python",
   "name": "llm_projects"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
