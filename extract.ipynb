{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c68e274-9e22-4218-977c-512ea3685ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\1\\ipykernel_11020\\2765855040.py:29: LangChainDeprecationWarning: The class `ChatAnthropic` was deprecated in LangChain 0.0.28 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-anthropic package and should be used instead. To use it run `pip install -U :class:`~langchain-anthropic` and import as `from :class:`~langchain_anthropic import ChatAnthropic``.\n",
      "  return ChatAnthropic(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Anthropic' object has no attribute 'count_tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 237\u001b[0m\n\u001b[0;32m    234\u001b[0m     process_files(config)\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 237\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 234\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m    233\u001b[0m     config \u001b[38;5;241m=\u001b[39m load_config()\n\u001b[1;32m--> 234\u001b[0m     \u001b[43mprocess_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 155\u001b[0m, in \u001b[0;36mprocess_files\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_files\u001b[39m(config):\n\u001b[0;32m    154\u001b[0m     s3_client \u001b[38;5;241m=\u001b[39m init_aws_client(config)\n\u001b[1;32m--> 155\u001b[0m     llm \u001b[38;5;241m=\u001b[39m \u001b[43minit_claude\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# List all files in the upload bucket/path\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     paginator \u001b[38;5;241m=\u001b[39m s3_client\u001b[38;5;241m.\u001b[39mget_paginator(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlist_objects_v2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 29\u001b[0m, in \u001b[0;36minit_claude\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minit_claude\u001b[39m(config):\n\u001b[1;32m---> 29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mChatAnthropic\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43manthropic_api_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43manthropic\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclaud_key\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclaude-3-sonnet-20240229\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\biu-llm-project-eWdI__U5-py3.11\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:216\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     emit_warning()\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\biu-llm-project-eWdI__U5-py3.11\\Lib\\site-packages\\langchain_core\\load\\serializable.py:125\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\biu-llm-project-eWdI__U5-py3.11\\Lib\\site-packages\\pydantic\\_internal\\_decorators_v1.py:148\u001b[0m, in \u001b[0;36mmake_v1_generic_root_validator.<locals>._wrapper1\u001b[1;34m(values, _)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapper1\u001b[39m(values: RootValidatorValues, _: core_schema\u001b[38;5;241m.\u001b[39mValidationInfo) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RootValidatorValues:\n\u001b[1;32m--> 148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvalidator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\biu-llm-project-eWdI__U5-py3.11\\Lib\\site-packages\\langchain_core\\utils\\pydantic.py:208\u001b[0m, in \u001b[0;36mpre_init.<locals>.wrapper\u001b[1;34m(cls, values)\u001b[0m\n\u001b[0;32m    205\u001b[0m             values[name] \u001b[38;5;241m=\u001b[39m field_info\u001b[38;5;241m.\u001b[39mdefault\n\u001b[0;32m    207\u001b[0m \u001b[38;5;66;03m# Call the decorated function\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\biu-llm-project-eWdI__U5-py3.11\\Lib\\site-packages\\langchain_community\\llms\\anthropic.py:108\u001b[0m, in \u001b[0;36m_AnthropicCommon.validate_environment\u001b[1;34m(cls, values)\u001b[0m\n\u001b[0;32m    106\u001b[0m     values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHUMAN_PROMPT\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m anthropic\u001b[38;5;241m.\u001b[39mHUMAN_PROMPT\n\u001b[0;32m    107\u001b[0m     values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAI_PROMPT\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m anthropic\u001b[38;5;241m.\u001b[39mAI_PROMPT\n\u001b[1;32m--> 108\u001b[0m     values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclient\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount_tokens\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m    112\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import anthropic python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    113\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease it install it with `pip install anthropic`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    114\u001b[0m     )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Anthropic' object has no attribute 'count_tokens'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import boto3\n",
    "from langchain.llms import Anthropic\n",
    "from langchain_community.chat_models import ChatAnthropic\n",
    "import pytesseract\n",
    "import pdf2image\n",
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "import io\n",
    "import tempfile\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def load_config():\n",
    "    with open('config/config.yaml', 'r') as file:\n",
    "        return yaml.safe_load(file)\n",
    "\n",
    "def init_aws_client(config):\n",
    "    return boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id=config['aws']['aws_access_key_id'],\n",
    "        aws_secret_access_key=config['aws']['aws_secret_access_key'],\n",
    "        region_name=config['aws']['region_name']\n",
    "    )\n",
    "\n",
    "def init_claude(config):\n",
    "    return ChatAnthropic(\n",
    "        anthropic_api_key=config['anthropic']['claud_key'],\n",
    "        model=\"claude-3-sonnet-20240229\"\n",
    "    )\n",
    "\n",
    "def download_file(s3_client, bucket, key):\n",
    "    with tempfile.NamedTemporaryFile(delete=False) as tmp_file:\n",
    "        s3_client.download_file(bucket, key, tmp_file.name)\n",
    "        return tmp_file.name\n",
    "\n",
    "def extract_text_from_image(image):\n",
    "    \"\"\"Extract text from a PIL Image object using Tesseract with Hebrew support\"\"\"\n",
    "    # Convert PIL Image to OpenCV format\n",
    "    opencv_img = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # Image preprocessing\n",
    "    gray = cv2.cvtColor(opencv_img, cv2.COLOR_BGR2GRAY)\n",
    "    denoised = cv2.fastNlMeansDenoising(gray)\n",
    "    \n",
    "    # Adaptive thresholding\n",
    "    thresh = cv2.adaptiveThreshold(\n",
    "        denoised, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "        cv2.THRESH_BINARY, 11, 2\n",
    "    )\n",
    "    \n",
    "    # Convert back to PIL Image\n",
    "    processed_img = Image.fromarray(thresh)\n",
    "    \n",
    "    # Extract text with Tesseract, supporting both Hebrew and English\n",
    "    text = pytesseract.image_to_string(\n",
    "        processed_img, \n",
    "        lang='eng+heb',  # Use both English and Hebrew languages\n",
    "        config='--psm 3'  # Fully automatic page segmentation\n",
    "    )\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text and images from PDF\"\"\"\n",
    "    all_text = []\n",
    "    \n",
    "    # Open PDF with PyMuPDF\n",
    "    doc = fitz.open(pdf_path)\n",
    "    \n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc[page_num]\n",
    "        \n",
    "        # Extract text directly from PDF\n",
    "        text = page.get_text()\n",
    "        all_text.append(text)\n",
    "        \n",
    "        # Extract images from PDF\n",
    "        image_list = page.get_images()\n",
    "        \n",
    "        for img_index, img in enumerate(image_list):\n",
    "            try:\n",
    "                # Get image data\n",
    "                xref = img[0]\n",
    "                base_image = doc.extract_image(xref)\n",
    "                image_bytes = base_image[\"image\"]\n",
    "                \n",
    "                # Convert to PIL Image\n",
    "                image = Image.open(io.BytesIO(image_bytes))\n",
    "                \n",
    "                # Extract text from image\n",
    "                image_text = extract_text_from_image(image)\n",
    "                if image_text:\n",
    "                    all_text.append(image_text)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting image {img_index} from page {page_num}: {str(e)}\")\n",
    "                \n",
    "    doc.close()\n",
    "    return \"\\n\".join(all_text)\n",
    "\n",
    "def extract_text_from_image_file(image_path):\n",
    "    \"\"\"Extract text from image file\"\"\"\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            return extract_text_from_image(img)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image file: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "def analyze_with_claude(llm, text):\n",
    "    prompt = \"\"\"\n",
    "    Analyze this document and extract the following information in JSON format.\n",
    "    The text contains course syllabus information, likely in both Hebrew and English.\n",
    "    \n",
    "    Required JSON structure:\n",
    "    {\n",
    "        \"course_name\": \"Course name in original language\",\n",
    "        \"program_manager\": \"Look for 'מנהל התוכנית' or program manager\",\n",
    "        \"instructors\": [\n",
    "            {\n",
    "                \"name\": \"Instructor name\",\n",
    "                \"role\": \"Role (e.g., יועץ מקצועי, מרצה, מרצה בכיר, מדריך)\",\n",
    "                \"title\": \"Professional title if available\",\n",
    "                \"description\": \"Additional description or background\"\n",
    "            }\n",
    "        ],\n",
    "        \"summary\": \"A comprehensive summary of the course content\"\n",
    "    }\n",
    "\n",
    "    Keep all text in its original language (Hebrew or English).\n",
    "    Ensure proper extraction of Hebrew text and names.\n",
    "    If a field is not found, use null or empty array [].\n",
    "    \n",
    "    Document text:\n",
    "    {text}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = llm.complete(prompt.format(text=text[:15000]))  # Limit text length\n",
    "        return json.loads(response.completion)\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing with Claude: {str(e)}\")\n",
    "        return {\n",
    "            \"course_name\": \"\",\n",
    "            \"program_manager\": \"\",\n",
    "            \"instructors\": [],\n",
    "            \"summary\": \"Error analyzing content\"\n",
    "        }\n",
    "\n",
    "def process_files(config):\n",
    "    s3_client = init_aws_client(config)\n",
    "    llm = init_claude(config)\n",
    "    \n",
    "    # List all files in the upload bucket/path\n",
    "    paginator = s3_client.get_paginator('list_objects_v2')\n",
    "    pages = paginator.paginate(\n",
    "        Bucket=config['aws']['upload_bucket_name'],\n",
    "        Prefix=config['aws']['upload_path']\n",
    "    )\n",
    "\n",
    "    for page in pages:\n",
    "        for obj in page.get('Contents', []):\n",
    "            file_key = obj['Key']\n",
    "            file_name = os.path.basename(file_key)\n",
    "            file_ext = os.path.splitext(file_name)[1][1:].lower()\n",
    "            \n",
    "            if file_ext not in ['pdf', 'jpg', 'jpeg', 'png']:\n",
    "                continue\n",
    "\n",
    "            print(f\"Processing file: {file_name}\")\n",
    "            \n",
    "            # Download file\n",
    "            temp_path = download_file(\n",
    "                s3_client, \n",
    "                config['aws']['upload_bucket_name'], \n",
    "                file_key\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                # Extract text based on file type\n",
    "                if file_ext == 'pdf':\n",
    "                    extracted_text = extract_text_from_pdf(temp_path)\n",
    "                else:\n",
    "                    extracted_text = extract_text_from_image_file(temp_path)\n",
    "                \n",
    "                # Analyze with Claude\n",
    "                analysis = analyze_with_claude(llm, extracted_text)\n",
    "                \n",
    "                # Create result JSON\n",
    "                result = {\n",
    "                    \"file_name\": file_name,\n",
    "                    \"course_name\": analysis.get(\"course_name\", \"\"),\n",
    "                    \"program_manager\": analysis.get(\"program_manager\", \"\"),\n",
    "                    \"instructors\": analysis.get(\"instructors\", []),\n",
    "                    \"summary\": analysis.get(\"summary\", \"\"),\n",
    "                    \"text\": extracted_text\n",
    "                }\n",
    "\n",
    "                # Ensure proper instructor structure\n",
    "                for instructor in result[\"instructors\"]:\n",
    "                    if isinstance(instructor, dict):\n",
    "                        instructor.setdefault(\"name\", \"\")\n",
    "                        instructor.setdefault(\"role\", \"\")\n",
    "                        instructor.setdefault(\"title\", None)\n",
    "                        instructor.setdefault(\"description\", None)\n",
    "\n",
    "                # Save to S3\n",
    "                json_key = os.path.join(\n",
    "                    config['aws']['extract_txt_path'],\n",
    "                    f\"{os.path.splitext(file_name)[0]}.json\"\n",
    "                )\n",
    "                \n",
    "                s3_client.put_object(\n",
    "                    Bucket=config['aws']['txt_extract_bucket_name'],\n",
    "                    Key=json_key,\n",
    "                    Body=json.dumps(result, ensure_ascii=False, indent=2).encode('utf-8')\n",
    "                )\n",
    "\n",
    "                print(f\"Successfully processed and saved: {file_name}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_name}: {str(e)}\")\n",
    "            \n",
    "            finally:\n",
    "                # Cleanup\n",
    "                if os.path.exists(temp_path):\n",
    "                    os.remove(temp_path)\n",
    "\n",
    "def main():\n",
    "    config = load_config()\n",
    "    process_files(config)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4b2b71-4b44-4715-a5ec-9d1a67d9fc48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_Projects",
   "language": "python",
   "name": "llm_projects"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
