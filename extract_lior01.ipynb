{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bda5405d-2cc9-4c41-b872-7fd56fd871e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "\n",
    "import yaml\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "import boto3\n",
    "from botocore.exceptions import NoCredentialsError, ClientError\n",
    "\n",
    "import base64  # Added base64 import\n",
    "from PIL import Image\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37bc4f8a-05a8-485f-89fe-9c5e1f3222ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\github_repos\\BIU_LLM_Project\n"
     ]
    }
   ],
   "source": [
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c28c3161-e528-4578-b200-2e8f0d5c0061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config():\n",
    "    \"\"\"Load configuration from YAML file\"\"\"\n",
    "    with open('config/config.yaml', 'r') as file:\n",
    "        return yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "102f9d74-dd42-4cfb-9419-b97412ce25ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_aws_client(config):\n",
    "    \"\"\"Initialize AWS S3 client\"\"\"\n",
    "    return boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id=config['aws']['aws_access_key_id'],\n",
    "        aws_secret_access_key=config['aws']['aws_secret_access_key'],\n",
    "        region_name=config['aws']['region_name']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee244e6e-79cd-4692-a4f6-3af9ac46fdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_s3_public_url(bucket_name, file_key, region):\n",
    "    \"\"\"Generate S3 public URL\"\"\"\n",
    "    return f\"https://{bucket_name}.s3.{region}.amazonaws.com/{file_key}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c672053e-8af1-467d-9f9d-90975299f634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_tmp_dir():\n",
    "    \"\"\"Create and clean temporary directory\"\"\"\n",
    "    tmp_dir = os.path.join(os.getcwd(), 'tmp')\n",
    "    if not os.path.exists(tmp_dir):\n",
    "        os.makedirs(tmp_dir)\n",
    "    else:\n",
    "        for filename in os.listdir(tmp_dir):\n",
    "            file_path = os.path.join(tmp_dir, filename)\n",
    "            try:\n",
    "                if os.path.isfile(file_path):\n",
    "                    os.unlink(file_path)\n",
    "            except Exception as e:\n",
    "                print(f'Error deleting {file_path}: {e}')\n",
    "    return tmp_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad029fff-8a65-49fc-a66e-238b27170eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Extract both regular text and text from images in an input file.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Open the PDF document with PyMuPDF (fitz)\n",
    "    doc = fitz.open(file_path)\n",
    "    text_content = []  # List to store extracted text and image placeholders\n",
    "    \n",
    "    # Loop through each page in the document\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc[page_num]  # Access a specific page in the PDF\n",
    "        \n",
    "        # Extract regular text from the page and add to text content list\n",
    "        text_content.append(page.get_text())\n",
    "        \n",
    "        # Extract all images on the page\n",
    "        image_list = page.get_images()\n",
    "        for img_index, img in enumerate(image_list):\n",
    "            try:\n",
    "                # Get the image reference number (xref) to retrieve the image data\n",
    "                xref = img[0]\n",
    "                base_image = doc.extract_image(xref)  # Extracts image data (binary format)\n",
    "                image_bytes = base_image[\"image\"]  # Retrieves raw image bytes\n",
    "                \n",
    "                # Convert the image bytes to a PIL Image object (useful for OCR or further processing)\n",
    "                image = Image.open(io.BytesIO(image_bytes))  # Assumes PIL.Image and io are imported\n",
    "                \n",
    "                # Placeholder text to indicate an embedded image in the output\n",
    "                text_content.append(f\"[Embedded Image {page_num + 1}-{img_index + 1}]\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                # Print error message if image extraction fails\n",
    "                print(f\"Error extracting image {img_index} from page {page_num}: {str(e)}\")\n",
    "    \n",
    "    # Close the document to free up resources\n",
    "    doc.close()\n",
    "    \n",
    "    # Join all text content (including image placeholders) into a single string for output\n",
    "    return \"\\n\".join(text_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f00ce860-fae3-4039-b50e-878837c73d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_openai(config):\n",
    "    \"\"\"Initialize OpenAI API key from configuration.\"\"\"\n",
    "    openai.api_key = config['openai']['chat_gpt_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a5a1975-7bc6-4ad0-8afb-9d67d2d677a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_with_openai_eng(text):\n",
    "\n",
    "    \"\"\"Analyze document content using OpenAI's GPT-4 and return structured JSON data.\"\"\"\n",
    "    \n",
    "    # Initial processing prompt for the full text extraction and transformation\n",
    "    try:\n",
    "        chat_history = [\n",
    "            {\"role\": \"user\",\n",
    "             \"content\": \"\"\"\n",
    "                Analyze the following text started after @@@@.\n",
    "                This text is a course syllabus document that contains the course information.\n",
    "                Your mission is to process and extract the information in JSON file format.\n",
    "        \n",
    "                Required JSON structure: { \"full_text\": \"The complete text after processing\", \"full_text_no_tokens\": \"Number tokens that GPT-4 returns\" }\n",
    "        \n",
    "                HOW TO PROCESS THE FULL TEXT\n",
    "                1. Read the text very carefully.\n",
    "                2. Text may be in (Hebrew, English or mixed).\n",
    "                3. Preserve all language terms (Hebrew, English, or mixed) in their original language.\n",
    "                4. You can use all available response tokens for output.\n",
    "            \n",
    "                5. For the full_text field, apply these transformations:\n",
    "                   - Add topic sentences to each major section.\n",
    "                   - Add clear section headers in capital letters.\n",
    "                   - Normalize spacing and formatting.\n",
    "                   - End all sentences with proper punctuation.\n",
    "                   - Transform bullet points into complete sentences.\n",
    "                   - Create proper paragraphs from lists.\n",
    "                   - Add contextual transitions between sections.\n",
    "                   - Ensure hierarchical content structure.\n",
    "                   - Preserve important dates, numbers, and contact information.\n",
    "                   - Remove duplicate headers/footers.\n",
    "                   - Remove any text that says: \"[Embedded Image ]\" or similar.\n",
    "                   - Maintain consistent formatting throughout.\n",
    "    \n",
    "                6. Structure the text in sections as follows:\n",
    "                   - COURSE NAME\n",
    "                   - COURSE OVERVIEW\n",
    "                   - TARGET AUDIENCE\n",
    "                   - COURSE LEADERSHIP\n",
    "                   - PREREQUISITES\n",
    "                   - CERTIFICATIONS AWARDED\n",
    "                   - COURSE SCHEDULE\n",
    "                   - LEARNING FORMAT\n",
    "                   - TUITION AND FEES\n",
    "                   - COURSE TOPICS\n",
    "                   - REGISTRATION AND CONTACT INFORMATION\n",
    "                   - INSTITUTIONAL INFORMATION\n",
    "    \n",
    "                7. Writing style guidelines:\n",
    "                   - Use complete sentences.\n",
    "                   - Add context to technical terms.\n",
    "                   - Maintain consistent tense and voice.\n",
    "                   - Ensure clarity and readability.\n",
    "                   - Preserve all relevant information from the original.\n",
    "    \n",
    "                8. Formatting requirements:\n",
    "                   - Use consistent paragraph spacing.\n",
    "                   - Ensure proper nesting of information.\n",
    "                   - Keep important identifiers (emails, URLs, phone numbers) in their original format.\n",
    "            \n",
    "                Text to analyze after this @@@@\n",
    "                \"\"\"\n",
    "            }\n",
    "        ]\n",
    "   \n",
    "        client = OpenAI()\n",
    "        \n",
    "        # Send prompt01 to OpenAI GPT-4 for initial processing\n",
    "        response = client.chat.completion.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {f\"{chat_history} \\n {text}\"}\n",
    "            ],\n",
    "            max_tokens=4000,\n",
    "            temperature=0  # Set temperature to 0 for deterministic output\n",
    "        )\n",
    "        \n",
    "        # Extract response text from prompt01\n",
    "        response_text1 = response.choices[0].message.content.strip()\n",
    "        usage = response['usage']\n",
    "        # Extract JSON structure from the response of prompt01\n",
    "        start = response_text1.find('{')\n",
    "        end = response_text1.rfind('}') + 1\n",
    "        \n",
    "        if start != -1 and end != 0:\n",
    "            json_str1 = response_text1[start:end]\n",
    "            initial_data = json.loads(json_str1)\n",
    "            response_tokens = usage['completion_tokens']\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"No JSON found in response\")\n",
    "\n",
    "        # Second prompt for detailed extraction based on the initial processed text\n",
    "\n",
    "        chat_history = [\n",
    "            {\"role\": \"user\",\n",
    "             \"content\": \"\"\"           \n",
    "                From the following course text after @@@@, extract these specific details:\n",
    "                1. The complete course name exactly as it appears\n",
    "                \n",
    "                2. From COURSE LEADERSHIP section:\n",
    "                   - Create a LIST of ALL instructors, including every teaching staff member mentioned\n",
    "                   - For each instructor in the list, extract:\n",
    "                     * Full name as written\n",
    "                     * Their role/position (e.g., יועץ מקצועי, מרצה, מרצה בכיר, מדריך)\n",
    "                     * Professional title (if mentioned)\n",
    "                     * Any additional description or background information\n",
    "                   - Make sure to capture EVERY instructor mentioned, even if some details are missing\n",
    "                   - List format should handle multiple instructors\n",
    "                \n",
    "                3. From the entire text:\n",
    "                   - Create a comprehensive course summary\n",
    "                   - Do not use text from INSTITUTIONAL INFORMATION or motto.\n",
    "                \n",
    "                Return the information in this exact JSON format:\n",
    "                {\n",
    "                    \"course_name\": \"Complete course name as it appears\",\n",
    "                    \"instructors\": [\n",
    "                        {\n",
    "                            \"name\": \"First instructor name\",\n",
    "                            \"role\": \"Role (e.g., יועץ מקצועי, מרצה, מרצה בכיר, מדריך)\",\n",
    "                            \"title\": \"Professional title if available\",\n",
    "                            \"description\": \"Additional description\"\n",
    "                        }\n",
    "                        // Continue for all instructors found\n",
    "                    ],\n",
    "                    \"summary\": \"Comprehensive course summary\"\n",
    "                }\n",
    "        \n",
    "                Important notes:\n",
    "                1. Keep all text in its original language (Hebrew and English)\n",
    "                2. Use null for missing fields\n",
    "                3. Use empty array [] if no instructors are found\n",
    "                4. Include ALL instructors mentioned in the text\n",
    "                5. Maintain list format even if only one instructor is found\n",
    "        \n",
    "                Text to analyze after this @@@@\n",
    "                \"\"\"\n",
    "            }\n",
    "        ]        \n",
    "        # Send prompt02 to OpenAI GPT-4 for detailed extraction\n",
    "        response = client.chat.completion.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {f\"{chat_history} \\n {initial_data.get('full_text', '')}\"}\n",
    "            ],\n",
    "            max_tokens=4000,\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        # Extract response text from prompt02\n",
    "        response_text2 = response.choices[0].message.content.strip()\n",
    "        extract_start = response_text2.find('{')\n",
    "        extract_end = response_text2.rfind('}') + 1\n",
    "        \n",
    "        if extract_start != -1 and extract_end != 0:\n",
    "            extracted_data = json.loads(response_text2[extract_start:extract_end])\n",
    "        else:\n",
    "            extracted_data = {\n",
    "                \"course_name\": None,\n",
    "                \"instructors\": [],\n",
    "                \"summary\": None\n",
    "            }\n",
    "\n",
    "        # Create final JSON output\n",
    "        final_json = {\n",
    "            \"course_name\": extracted_data.get(\"course_name\"),\n",
    "            \"instructors\": extracted_data.get(\"instructors\", []),\n",
    "            \"summary\": extracted_data.get(\"summary\"),\n",
    "            \"full_text\": initial_data.get(\"full_text\", text),\n",
    "            \"full_text_no_tokens\": response_tokens\n",
    "        }\n",
    "\n",
    "        return final_json\n",
    "\n",
    "    except (json.JSONDecodeError, ValueError) as e:\n",
    "        print(f\"JSON Parse Error: {str(e)}\")\n",
    "        print(f\"Response text: {response_text1 if 'response_text1' in locals() else response_text2}\")\n",
    "        return {\n",
    "            \"course_name\": None,\n",
    "            \"instructors\": [],\n",
    "            \"summary\": None,\n",
    "            \"full_text\": text,\n",
    "            \"full_text_no_tokens\": 0\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing with OpenAI: {str(e)}\")\n",
    "        return {\n",
    "            \"course_name\": None,\n",
    "            \"instructors\": [],\n",
    "            \"summary\": None,\n",
    "            \"full_text\": text,\n",
    "            \"full_text_no_tokens\": 0\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1a35645-c648-4997-b481-b3129cab272c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_claude(config):\n",
    "    return Anthropic(\n",
    "        api_key=config['anthropic']['claud_key']\n",
    "    )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e53d3312-ebf9-4875-a7dc-96246d530626",
   "metadata": {},
   "source": [
    "def analyze_with_claude_eng(client, text):\n",
    "    \"\"\"Analyze document content using Claude and return structured JSON data.\"\"\"\n",
    "    \n",
    "    prompt01 = \"\"\"\n",
    "        Analyze the following text started after @@@@.\n",
    "        This text is a course syllabus document that contains the course information.\n",
    "        Your mission is to process and extract the information in JSON file format.\n",
    "        \n",
    "        Required JSON structure: { \"full_text\": \"The complete text after processing\", \"full_text_no_tokens\": \"Number tokens that claude return\" }\n",
    "       \n",
    "        HOW TO PROCESS THE FULL TEXT\n",
    "        1. Read the text very carefully.\n",
    "        2. Text may be in (Hebrew, English or mixed).\n",
    "        2. Preserve all languages terms (Hebrew, English, or mixed) in their original language.\n",
    "        3. You can use all available response tokens for output.\n",
    "            \n",
    "        4. For the full_text field, apply these transformations:\n",
    "          - Add topic sentences to each major section.\n",
    "          - Add clear section headers in capital letters.\n",
    "          - Normalize spacing and formatting.\n",
    "          - End all sentences with proper punctuation. Remeber to put \".\" at the end of each sentence and bullet.\n",
    "          - Transform bullet points into complete sentences.\n",
    "          - Create proper paragraphs from lists.\n",
    "          - Add contextual transitions between sections.\n",
    "          - Ensure hierarchical content structure.         \n",
    "          - Preserve important dates, numbers, and contact information.\n",
    "          - Include clear semantic markers for key concepts.       \n",
    "          - Remove duplicate headers/footers.\n",
    "          - Remove any text says: \"[Embedded Image ]\" or like.\n",
    "          - Maintain consistent formatting throughout.\n",
    "    \n",
    "       5. Structure the text in sections\n",
    "          - COURSE NAME: Course name or workshop name.\n",
    "          - COURSE OVERVIEW: course overview, or the program description.\n",
    "          - TARGET AUDIENCE: if there are.if not found, skip this section\n",
    "          - COURSE LEADERSHIP: Note for: Academic Director, Academic Leaders and their roles, Academic Advisors and Lecturers - Include all leaders biography and their descriptions. if not found, skip this section.\n",
    "          - PREREQUISITES: if there are. if not found, skip this section\n",
    "          - CERTIFICATIONS AWARDED: if there are. if not found, skip this section.\n",
    "          - COURSE SCHEDULE: look for start date, end date, duration, schedules. be bullet focused.\n",
    "          - LEARNING FORMAT: look for things like location, in class, remote or hybrid learning options, laboratories, practicum, staj, etc. - if there are: be bullet focused. if not found, skip this section.\n",
    "          - TUITION AND FEES: look for registration fees, tuition, parking fess. if there are: be bullet focused. if not found, skip this section\n",
    "          - COURSE TOPICS: Bring all topics that appear in text. SUPER IMPORTANT TO BE DETAILED. DO NOT OMIT ANYTHING. You may orgenise topics in sections, sub-sections and under bullets.\n",
    "          - REGISTRATION AND CONTACT INFORMATION: if there are: be bullet focused. if not found, skip this section\n",
    "          - INSTITUTIONAL INFORMATION: Focused institutional information. if there are: be focused. if not found, skip this section\n",
    "       \n",
    "       6. Writing style guidelines:\n",
    "          - Use complete sentences.\n",
    "          - Add context to technical terms.\n",
    "          - Create smooth transitions between topics.\n",
    "          - Maintain consistent tense and voice.\n",
    "          - Ensure clarity and readability.\n",
    "          - Preserve all relevant information from the original.\n",
    "    \n",
    "       7. Formatting requirements:\n",
    "          - Use consistent paragraph spacing.\n",
    "          - Maintain clear section boundaries.\n",
    "          - Ensure proper nesting of information.\n",
    "          - Use standard punctuation.\n",
    "          - Keep important identifiers (emails, URLs, phone numbers) in their original format.\n",
    "    \n",
    "       The output JSON should maintain the same JSON structure but with the full_text field optimized for RAG applications while preserving all original information.\n",
    "    \n",
    "        @@@@\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Initial Claude call and response handling remains the same\n",
    "        message = client.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20241022\",\n",
    "            max_tokens=8192,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt01\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": text\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        response_text = message.content[0].text.strip()\n",
    "        response_tokens = message.usage.output_tokens\n",
    "        \n",
    "        # Extract initial JSON\n",
    "        start = response_text.find('{')\n",
    "        end = response_text.rfind('}') + 1\n",
    "        \n",
    "        if start != -1 and end != 0:\n",
    "            json_str = response_text[start:end]\n",
    "            claude_response = json.loads(json_str)\n",
    "            \n",
    "            # Modified extraction prompt with explicit list format instruction\n",
    "            \n",
    "            prompt2 = \"\"\"\n",
    "            From the following course text after @@@@, extract these specific details:\n",
    "\n",
    "            1. The complete course name exactly as it appears\n",
    "            \n",
    "            2. From COURSE LEADERSHIP section:\n",
    "               - Create a LIST of ALL instructors, including every teaching staff member mentioned\n",
    "               - For each instructor in the list, extract:\n",
    "                 * Full name as written\n",
    "                 * Their role/position (e.g., יועץ מקצועי, מרצה, מרצה בכיר, מדריך)\n",
    "                 * Professional title (if mentioned)\n",
    "                 * Any additional description or background information\n",
    "               - Make sure to capture EVERY instructor mentioned, even if some details are missing\n",
    "               - List format should handle multiple instructors\n",
    "            \n",
    "            3. From the entire text:\n",
    "               - Create a comprehensive course summary\n",
    "               - Do not use text from  INSTITUTIONAL INFORMATION: or motto. \n",
    "            \n",
    "            Return the information in this exact JSON format:\n",
    "            {\n",
    "                \"course_name\": \"Complete course name as it appears\",\n",
    "                \"instructors\": [\n",
    "                    {\n",
    "                        \"name\": \"First instructor name\",\n",
    "                        \"role\": \"Role (e.g., יועץ מקצועי, מרצה, מרצה בכיר, מדריך)\",\n",
    "                        \"title\": \"Professional title if available\",\n",
    "                        \"description\": \"Additional description\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"Second instructor name\",\n",
    "                        \"role\": \"Their role\",\n",
    "                        \"title\": \"Their title\",\n",
    "                        \"description\": \"Their description\"\n",
    "                    }\n",
    "                    // Continue for all instructors found\n",
    "                ],\n",
    "                \"summary\": \"Comprehensive course summary\"\n",
    "            }\n",
    "            \n",
    "            Important notes:\n",
    "            1. Keep all text in its original language (Hebrew and English)\n",
    "            2. Use null for missing fields\n",
    "            3. Use empty array [] if no instructors are found\n",
    "            4. Include ALL instructors mentioned in the text\n",
    "            5. Maintain list format even if only one instructor is found\n",
    "            \n",
    "            Text to anlyse after this @@@@\n",
    "            \"\"\"\n",
    "            \n",
    "            # Send second prompt to Claude\n",
    "            extraction_message = client.messages.create(\n",
    "                model=\"claude-3-5-sonnet-20241022\",\n",
    "                max_tokens=8192,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": prompt2\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": claude_response.get(\"full_text\", \"\")\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            # Parse extraction response\n",
    "            extraction_text = extraction_message.content[0].text.strip()\n",
    "            extract_start = extraction_text.find('{')\n",
    "            extract_end = extraction_text.rfind('}') + 1\n",
    "            \n",
    "            if extract_start != -1 and extract_end != 0:\n",
    "                extracted_data = json.loads(extraction_text[extract_start:extract_end])\n",
    "            else:\n",
    "                extracted_data = {\n",
    "                    \"course_name\": None,\n",
    "                    \"instructors\": [],\n",
    "                    \"summary\": None\n",
    "                }\n",
    "            \n",
    "            # Create final complete JSON output\n",
    "            final_json = {\n",
    "                \"course_name\": extracted_data.get(\"course_name\"),\n",
    "                \"instructors\": extracted_data.get(\"instructors\", []),\n",
    "                \"summary\": extracted_data.get(\"summary\"),\n",
    "                \"full_text\": claude_response.get(\"full_text\", text),\n",
    "                \"full_text_no_tokens\": int(response_tokens)\n",
    "            }\n",
    "            \n",
    "            return final_json\n",
    "\n",
    "    except (json.JSONDecodeError, ValueError) as e:\n",
    "        print(f\"JSON Parse Error: {str(e)}\")\n",
    "        print(f\"Response text: {response_text}\")\n",
    "        return {\n",
    "            \"course_name\": None,\n",
    "            \"instructors\": [],\n",
    "            \"summary\": None,\n",
    "            \"full_text\": text,\n",
    "            \"full_text_no_tokens\": 0\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing with Claude: {str(e)}\")\n",
    "        return {\n",
    "            \"course_name\": None,\n",
    "            \"instructors\": [],\n",
    "            \"summary\": None,\n",
    "            \"full_text\": text,\n",
    "            \"full_text_no_tokens\": 0\n",
    "        }"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c2ccb27b-de78-4254-a6b9-032772249977",
   "metadata": {},
   "source": [
    "def analyze_with_claude_heb(client, text):\n",
    "    \"\"\"Analyze document content using Claude and return structured JSON data in Hebrew.\"\"\"\n",
    "    \n",
    "    # Prompt for Hebrew output while preserving professional terms\n",
    "    prompt01 = \"\"\"\n",
    "        Analyze the following text started after @@@@.\n",
    "        This text is a course syllabus document that contains the course information.\n",
    "        Your mission is to process and extract the information in JSON file format.\n",
    "        \n",
    "        Required JSON structure: { \"full_text\": \"The complete text after processing\", \"full_text_no_tokens\": \"Number tokens that claude return\" }\n",
    "       \n",
    "        HOW TO PROCESS THE FULL TEXT\n",
    "        1. Read the text very carefully.\n",
    "        2. Text may be in (Hebrew, English or mixed).\n",
    "        3. OUTPUT MUST BE IN HEBREW with these guidelines:\n",
    "           - Keep technical or professional terms, course names, and technical concepts in their original language\n",
    "           - Keep proper names in their original form\n",
    "           - Keep email addresses, phone numbers, and URLs in their original format\n",
    "           - Translate all descriptive and contextual content to Hebrew\n",
    "           - Use Hebrew punctuation marks where appropriate (including period at sentence end)\n",
    "           - Format numbers according to Hebrew convention\n",
    "           - Maintain Right-to-Left (RTL) formatting\n",
    "           \n",
    "        4. You can use all available response tokens for output.\n",
    "            \n",
    "        5. For the full_text field, apply these transformations:\n",
    "          - הוסף משפטי נושא לכל חלק עיקרי\n",
    "          - הוסף כותרות ברורות באותיות גדולות (באנגלית) או מודגשות (בעברית)\n",
    "          - נרמל רווחים ועיצוב\n",
    "          - סיים את כל המשפטים בסימני פיסוק מתאימים\n",
    "          - הפוך נקודות עם תבליטים למשפטים שלמים\n",
    "          - צור פסקאות תקינות מרשימות\n",
    "          - הוסף מעברים הקשריים בין חלקים\n",
    "          - ודא מבנה תוכן היררכי\n",
    "          - שמור על תאריכים, מספרים ופרטי קשר חשובים\n",
    "          - כלול סמנים סמנטיים ברורים למושגי מפתח\n",
    "          - הסר כותרות עליונות/תחתונות כפולות\n",
    "          - \" [Embedded Image] \" הסר כל טקסט כגון\n",
    "          - שמור על עיצוב עקבי לאורך כל הטקסט\n",
    "          \n",
    "    \n",
    "       6. מבנה הטקסט בסעיפים\n",
    "          - שם הקורס: שם הקורס המקורי. להישאר ממוקד\n",
    "          - סקירת הקורס: סקירת הקורס או תיאור התוכנית\n",
    "          - קהל יעד: אם יש. אם אין, דלג על חלק זה\n",
    "          - מובילי הקורס: הערה עבור: מנהל אקדמי, מובילים אקדמיים ונקודות המיקוד שלהם, יועצים אקדמיים ומרצים. בנוסף הבא את התאירו עליהם. אם יש. אם אין, דלג על חלק זה\n",
    "          - דרישות קדם אם יש. אם אין, דלג על חלק זה\n",
    "          - תעודות: אם יש. אם אין, דלג על חלק זה\n",
    "          - לוח זמנים: חפש תאריך התחלה, תאריך סיום, משך, לוחות זמנים. התמקד בנקודות\n",
    "          - פורמט למידה: חפש דברים כמו מיקום, בכיתה, למידה מרחוק או היברידית, מעבדות, התמחות מעשית וכו'. אם יש: התמקד בנקודות. אם אין, דלג על חלק זה\n",
    "          - שכר לימוד ותשלום: חפש דמי רישום, שכר לימוד, דמי חניה. אם יש: התמקד בנקודות. אם אין, דלג על חלק זה\n",
    "          - נושאי הקורס: הבא את כל הנושאים שמופיעים בטקסט. חשוב מאוד להיות מפורט. אל תשמיט דבר. ניתן לארגן נושאים בחלקים, תתי-חלקים ותחת תבליטים\n",
    "          - רישום ויצירת קשר: אם יש: התמקד בנקודות. אם אין, דלג על חלק זה\n",
    "          - מי אנחנו: מידע מוסדי ממוקד. אם יש: התמקד. אם אין, דלג על חלק זה\n",
    "       \n",
    "       7. הנחיות סגנון כתיבה:\n",
    "          - השתמש במשפטים שלמים\n",
    "          - הוסף הקשר למונחים טכניים\n",
    "          - צור מעברים חלקים בין נושאים\n",
    "          - שמור על עקביות בזמן ובקול\n",
    "          - הבטח בהירות וקריאות\n",
    "          - שמור על כל המידע הרלוונטי מהמקור\n",
    "    \n",
    "       8. דרישות עיצוב:\n",
    "          - השתמש ברווח עקבי בין פסקאות\n",
    "          - שמור על גבולות ברורים בין חלקים\n",
    "          - הבטח קינון מידע תקין\n",
    "          - השתמש בפיסוק תקני\n",
    "          - שמור על מזהים חשובים (אימיילים, מספרי טלפון, כתובות אינטרנט) בפורמט המקורי שלהם\n",
    "    \n",
    "       The output JSON should maintain the same JSON structure but with the full_text field optimized for RAG applications while preserving all original information.\n",
    "    \n",
    "        @@@@\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Send the prompt to Claude and get the response\n",
    "        message = client.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20241022\",\n",
    "            max_tokens=8192,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt01\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": text\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Extract and clean up the response text\n",
    "        response_text   = message.content[0].text.strip()\n",
    "        response_tokens = message.usage.output_tokens\n",
    "        \n",
    "        # Try to locate and extract JSON structure from Claude's response\n",
    "        start = response_text.find('{')\n",
    "        end = response_text.rfind('}') + 1\n",
    "        if start != -1 and end != 0:\n",
    "            json_str = response_text[start:end]\n",
    "            claude_response = json.loads(json_str)\n",
    "        else:\n",
    "            raise ValueError(\"No JSON found in response\")\n",
    "       \n",
    "        # Create ordered dictionary for the structured response\n",
    "        ordered_response1 = OrderedDict([\n",
    "            (\"full_text\", claude_response.get(\"full_text\", text)),\n",
    "            (\"full_text_no_tokens\", int(response_tokens))  \n",
    "        ])\n",
    "\n",
    "        return ordered_response1\n",
    "\n",
    "    except (json.JSONDecodeError, ValueError) as e:\n",
    "        print(f\"JSON Parse Error: {str(e)}\")\n",
    "        print(f\"Response text: {response_text}\")\n",
    "        return OrderedDict([\n",
    "            (\"full_text\", text),\n",
    "            (\"full_text_no_tokens\", 0)  # Return 0 for errors\n",
    "        ])\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing with Claude: {str(e)}\")\n",
    "        return OrderedDict([\n",
    "            (\"full_text\", text),\n",
    "            (\"full_text_no_tokens\", 0)  # Return 0 for errors\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a940b5d0-ec56-409e-b5ba-e7136f858a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(config):\n",
    "    \"\"\"Main processing function to handle file operations using AWS S3 and the Claude service.\"\"\"\n",
    "    \n",
    "    # Initialize AWS S3 client using provided configuration\n",
    "    s3_client = init_aws_client(config)\n",
    "\n",
    "    # Ensure the existence of a temporary directory for intermediate file storage\n",
    "    tmp_dir = ensure_tmp_dir()  \n",
    "           \n",
    "    # Set up the JSON output directory path in the 'data/documents' folder relative to the current working directory\n",
    "    json_output_dir_eng = os.path.join(os.getcwd(), 'data', 'documents', 'eng')\n",
    "    os.makedirs(json_output_dir_eng, exist_ok=True)  \n",
    "\n",
    "    # Set up the JSON output directory path in the 'data/documents' folder relative to the current working directory\n",
    "    json_output_dir_heb = os.path.join(os.getcwd(), 'data', 'documents', 'heb')\n",
    "    os.makedirs(json_output_dir_heb, exist_ok=True) \n",
    "    \n",
    "    # Initialize Claude client using provided configuration\n",
    "    claude_client = init_claude(config)\n",
    "\n",
    "    # Initialize Claude client using provided configuration\n",
    "    init_openai(config)\n",
    "\n",
    "    # Define supported file types to process\n",
    "    supported_file_types = tuple(config['local']['supported_file_types'])\n",
    "    \n",
    "    # List all files in AWS S3 upload bucket/path using the S3 paginator\n",
    "    paginator = s3_client.get_paginator('list_objects_v2')\n",
    "    pages = paginator.paginate(\n",
    "        Bucket=config['aws']['upload_bucket_name'],\n",
    "        Prefix=config['aws']['upload_path']\n",
    "    )\n",
    "\n",
    "    # Loop through all files in each page from the S3 bucket\n",
    "    for page in pages:\n",
    "        for obj in page.get('Contents', []):\n",
    "            s3_file_key = obj['Key']\n",
    "            file_name = os.path.basename(s3_file_key)\n",
    "            \n",
    "            # Process only supported file types\n",
    "            if not file_name.lower().endswith(supported_file_types):\n",
    "                continue\n",
    "\n",
    "            print(f\"\\nProcessing file: {file_name}\")\n",
    "            \n",
    "            # Create paths for the temporary PDF and output JSON\n",
    "            tmp_download_file_path = os.path.join(tmp_dir, file_name)\n",
    "\n",
    "            # Create output JSON english\n",
    "            json_filename = f\"{os.path.splitext(file_name)[0]}.json\"\n",
    "            local_json_path_eng = os.path.join(json_output_dir_eng, json_filename)\n",
    "\n",
    "            # Create output JSON hebrew\n",
    "            json_filename = f\"{os.path.splitext(file_name)[0]}.json\"\n",
    "            local_json_path_heb = os.path.join(json_output_dir_heb, json_filename)\n",
    "            \n",
    "            try:\n",
    "                # Download file to temporary location\n",
    "                s3_client.download_file(\n",
    "                    config['aws']['upload_bucket_name'], s3_file_key,\n",
    "                    tmp_download_file_path\n",
    "                )\n",
    "                \n",
    "                # Extract text from the file (including image references)\n",
    "                extracted_text = extract_text_from_file(tmp_download_file_path)\n",
    "                print(f\"\\nExtracted Text:\\n{extracted_text}\")\n",
    "\n",
    "                # Analyze text with Claude - english\n",
    "                # analysis_eng = analyze_with_claude_eng(claude_client, extracted_text)\n",
    "\n",
    "                # Analyze text with OpenAi - english\n",
    "                analysis_eng = analyze_with_openai_eng(extracted_text)\n",
    "                \n",
    "                print(f\"\\nLLM Analysis eng 1:\\n{analysis_eng}\")                      \n",
    "\n",
    "                # Save analysis to JSON output file\n",
    "                with open(local_json_path_eng, 'w') as json_file:\n",
    "                    json.dump(analysis_eng, json_file)\n",
    "\n",
    "                print(f\"Saved analysis to {local_json_path_eng}\")\n",
    "\n",
    "                '''\n",
    "                # Analyze text with Claude - hebrew\n",
    "                analysis_heb = analyze_with_claude_heb(claude_client, extracted_text)\n",
    "                print(f\"\\nLLM Analysis heb:\\n{analysis_heb}\")\n",
    "\n",
    "                # Save analysis to JSON output file\n",
    "                with open(local_json_path_heb, 'w') as json_file:\n",
    "                    json.dump(analysis_heb, json_file)\n",
    "\n",
    "                print(f\"Saved analysis to {local_json_path_heb}\")\n",
    "                '''\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_name}: {str(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32be2177-edf9-4fad-904d-edcbb540fa8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main process error: name 'openai' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'openai' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 12\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 5\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      4\u001b[0m     config \u001b[38;5;241m=\u001b[39m load_config()\n\u001b[1;32m----> 5\u001b[0m     \u001b[43mprocess_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMain process error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 22\u001b[0m, in \u001b[0;36mprocess_files\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m     19\u001b[0m claude_client \u001b[38;5;241m=\u001b[39m init_claude(config)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Initialize Claude client using provided configuration\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[43minit_openai\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Define supported file types to process\u001b[39;00m\n\u001b[0;32m     25\u001b[0m supported_file_types \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupported_file_types\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m, in \u001b[0;36minit_openai\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minit_openai\u001b[39m(config):\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Initialize OpenAI API key from configuration.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mopenai\u001b[49m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenai\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchat_gpt_key\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'openai' is not defined"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main entry point\"\"\"\n",
    "    try:\n",
    "        config = load_config()\n",
    "        process_files(config)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Main process error: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7f4533-80d3-43a9-86c3-df408bf9ff79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_Projects",
   "language": "python",
   "name": "llm_projects"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
